<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[墙下短记]]></title>
    <url>%2F2019%2F03%2F04%2Fqiang_xia_duan_ji%2F</url>
    <content type="text"><![CDATA[一些当时看去不太要紧的事却长久扎根在记忆里。他们一向都在那儿安睡，偶然醒一下，睁眼看看，见你忙着（升迁或者遁世）就又睡去。很多年里他们轻得仿佛不在。千百次机缘错过，终于一天又看见它们，看见时光把很多所谓人生大事消磨殆尽，而它们坚定不移固守在那儿，沉沉地有了无比的重量。比如一张旧日的照片，拍时并不经意，随手放在哪儿，多年中甚至不记得有它，可忽然一天整理旧物时碰见了，拂去尘埃，竟会感到那是你的由来也是你的投奔，而很多郑重其事的留影，却已忘记是在哪儿和为了什么。 近些年我常记起一道墙，碎砖头垒的，风可以吹落砖缝间的细土。那墙很长，至少在一个少年看来是很长，很长之后拐了弯，拐进一条更窄的小巷里去。小巷的拐角处有一盏街灯，紧挨着往前是一个院门，那里住过我少年时的一个同窗好友。叫他L吧。L和我能不能永远是好友并不重要，重要的是我们一度形影不离，我生命的一段就由这友谊铺筑。细密的小巷中，上学和放学的路上我们一起走，冬天或夏天，风声或蝉鸣，太阳到星空，十岁或者九岁的L曾对我说，他将来要娶班上一个女生（M）做老婆。L转身问我：“你呢？想和谁？”我准备不及，想想，觉得M也确是漂亮。L说他还要挣很多钱。“干吗？”“废话，那时你还花你爸的钱呀？”少年间的情谊，想来莫过于我们那时的无猜无防了。 我曾把一件珍爱的东西送给L。是什么，已经记不清。可是有一天我们打了架，为什么打架也记不清了，但丝毫不忘的是：打完架我去找L要回了那件东西。 老师说，单凭我一个人是不敢去要的，或者也想不起去要。是几个当时也对L不大满意的伙伴指点我、怂恿我，拍着胸脯说他们甘愿随我一同前去讨还，就去了。走过那道很长很熟悉的墙，夕阳正在上面灿烂地照耀，但在我的印象里，走到L家的院门时，巷角的街灯已经昏黄地亮了。不可能是那么长的墙，只可能是记忆作怪。 站在那门前，我有点害怕，身旁的伙伴便极尽动员和鼓励，提醒我：倘掉头撤退，其可卑甚至超过投降。我不能推罪责任给别人：跟L打架后，我为什么要把送给L东西的事情告诉别人呢？指点和怂恿都因此发生。我走进院中去喊L。L出来，听我说明来意，愣着看我一会儿，然后回屋那出那件东西交到我手里，不说什么，就又走回屋去。结束总是非常简单，咔嚓一下就都过去。 我和几个同来的伙伴在巷角的街灯下分手，各自回家。他们看看我手上那件东西，好歹说一句“给他干吗”，声调和表情都失去来时的热读，失望甚或沮丧料想都不由于那件东西。 我独自回家，贴近墙根走。墙很长，很长而且荒凉，记忆在这儿又出了差误，好像还是街灯未亮、迎面的行人眉目不清的时候。晚风轻柔得让人无可抱怨，但魂魄仿佛被它吹离，吹离身体，飘起在黄昏中再消失进那道墙里去。捡根树枝，边走边在墙上轻划，砖缝间的细土一股股地垂流……咔嚓一下所送走的，都扎根进记忆去酿制未来的问题。 那很可能是我对于墙的第一种印象。 随之，另一些墙也从睡中醒来。 有一天傍晚“散步”，我摇着轮椅走进童年时常于其间玩耍的一片胡同。其实一向都离它们不远，屡屡在其周围走过，匆忙得来不及进去看望。 记得那儿曾有一面红砖短墙，我们一群八九岁的孩子总去搅扰墙里那户人家的安宁，攀上一棵小树，扒着墙沿央告人家把我们的足球扔出来。那面墙应该说藏得很是隐蔽，在一条死巷里，但可惜那巷口的宽度很适合做我们的球门，巷口外的一片空地是我们的球场，球难免是要踢向球门的，倘临门一脚踢飞，十之八九便降落到那面墙里去。我们千般央告万般保证，揪心着阳光一会儿比一会儿暗淡，“球瘾”便又要熬磨一宿了。终于一天，那足球学着篮球的样子准确投入墙内的面锅，待一群孩子又爬上小树去看时，雪白的面条热气腾腾全滚在煤灰里。正是所谓“三年困难时期”，足球事小，我们乘暮色抱头鼠窜。几天后，我们由家长带领，以封闭“球场”为代价换回了那只足球。 那条小巷依旧，或者是更旧了。变化不多。惟独那片“球场”早被压在一家饭馆下面。红砖短墙里的人家料比是安全得多了。 我摇着轮椅走街串巷，忽然又一面青灰色的墙叫我砰然心动，我知道，再往前去就是我的幼儿园了。青灰色的墙很高，里面有更高的树。树顶上曾有鸟窝，现在没了。到幼儿园去必要经过这墙下，一俟见了这面墙，退步回家的希望即告断灭。 这样的“条件反射”确立于一个盛夏的午后，所以记得清楚，是因为那时的蝉鸣最为浩大。那个下午母亲要出差到很远的地方去。我最高的希望是她可能改变主意，最低的希望是我可以不去幼儿园，留在家里跟着奶奶。但两份提案均遭否决，据哭力争亦不奏效。如今想来，母亲是要在远行之前给我立下严明的纪律。哭声不停，母亲无奈说带我出去走走。“不去幼儿园！”出门时我再次申明立场。母亲领我在街上走，沿途买些好吃的东西给我，形式虽然可疑，但看看走了这么久又不像是去幼儿园的路，牵紧着母亲长裙的手遍放开，心里也略略地松坦。可是！好吃的东西刚在嘴里有了味道，迎头又来了那面青灰色高墙，才知道条条小路原来相通。虽立刻大哭，料已无济于事。但一迈进幼儿园的门槛，哭喊即自行停止，心里明白没了依靠，惟规规矩矩做个好孩子是得救的方略。幼儿园墙内，是必度的一种“灾难”，抑或只因为这一个孩子天生地怯懦和多愁。 三年前我搬了家，隔窗相望就是一所幼儿园，常在清晨的懒睡中就听见孩子进园前的嘶嚎。我特意去那园门前看过，抗拒进园的孩子其壮烈都像宁死不屈，但一落入园墙便立刻吞下哭声，恐惧变成冤屈，泪眼望天，抱紧着对晚霞的期待。不见得有谁比我更同情他们，但早早地对墙有一点感受，不是坏事。 我最记得母亲消失在那面青灰色高墙里的情景。她当然是绕过那面墙走上了远途的，但在我的印象里，她是走进那面墙里去了。没有门，但是母亲走进去了，在那里面，高高的树上蝉鸣浩大，高高的树下母亲的身影很小，在我的恐惧里那儿即是远方。 我现在有很多时间坐在窗前，看远近峭壁林立一般的高楼和矮墙。有人的地方一定有墙。我们都在墙里。没有多少事可以放心到光天化日下去做。 规规整整的高楼叫人想起图书馆的目录柜，只有上帝可以去拉开每一个小抽屉，查阅亿万种心灵秘史，看见破墙而出的梦想都在墙的封护中徘徊。还有死神按期来到，伸手进去，抓阄儿似的摸走几个。 我们有时千里迢迢——汽车呀、火车呀、飞机可别一头栽下来呀——只像是为了去找一处不见墙的地方：荒原、大海、林莽甚至沙漠。但未必就能逃脱。墙永久地在你心里，构筑恐惧，也牵动思念。比如你千里迢迢地去时，鲁宾逊正千里迢迢地回来。一只“飞去来器”，从墙出发，又回到墙。 哲学家先说是劳动创造了人，现在又说是语言创造了人。墙是否创造了人呢？语言和墙有着根本的相似：开不尽的门前是撞不尽的墙壁。结构呀、解构呀、后什么什么主义呀……啦啦啦，啦啦啦……游戏的热情永不可少，但我们仍在四壁的围阻中。把所有的墙都拆掉的愿望自古就有。不行么？我坐在窗前用很多时间去幻想一种魔法，比如“啦啦啦，啦啦啦……”很灵验地念上一段咒语，唰啦一下墙都不见。怎样呢？料必大家一齐慌作一团（就像热油淋在蚁穴），上哪儿的不知道要上哪儿了，干吗的忘记要干吗了，漫山遍野地捕食去和睡觉去么？毕竟又趣味不足。然后大家埋头细想，还是要砌墙。砌墙盖房，不单为避风雨，因为大家都有些秘密，其次当然还有一些钱财。秘密，不信你去慢慢推想，它是趣味的爹娘。 其实秘密就已经是墙了。肚皮和眼皮都是墙，假笑和伪哭都是墙，只因这样的墙嫌软嫌累，才要弄些坚实耐久的来。假设这心灵之墙可以轻易拆除，但山和水都是墙，天和地都是墙，时间和空间都是墙，命运是无穷的限制，上帝的秘密是不尽的墙，上帝所有的很可能就是造墙的智慧。真若把所有的墙都拆除，虽然很像似由来已久的理想接近了实现，但是等着瞧吧，满地球都怕要因为失去趣味而想起昏睡的鼾声，梦话亦不知从何说起。 趣味是要紧而又要紧的。秘密要好好保存。 探秘的欲望终于要探到意义的墙下。 活得要有意义，这老生常谈倒是任什么主义也不能推翻。加上个“后”字也是白搭。比如爱情，她能被物欲拐走一时，但不信她能因此绝灭。“什么都没啥了不起”的日子是要到头的，“什么都不必介意”的舞步可能“潇洒”地跳去撞墙。撞墙不死，第二步就是抬头，那时见墙上有字，写着：哥们儿你要上哪儿呢，这到底是要干吗？于是躲也躲不开，意义找上了门，债主的风度。 意义的原因很可能是意义本身。干吗要有意义？干吗要有生命？干吗要有存在？干吗要有有？重量的原因是引力，引力的原因呢？又是重量。学物理的告诉我们：千万别把运动和能量以及时空分割开来理解。我随即得了启发：也千万别把人和意义分割开来理解。不是人有欲望，而是人即欲望。这欲望就是能量，是能量就是运动，是运动就必走去前面或者未来。前面和未来都是什么和都是为什么？这必来的疑问使意义诞生，上帝便在第七天把人造成。上帝比靡菲斯特更有力量，任何魔法和咒语都不能把第七天的成就删除。在第七天以后的所有时光里，你逃得开某种意义，但逃不开意义，如同你逃得开一次旅行但你逃不开生命之旅。 你不是这种意义，就是那种意义。什么意义都不是，就掉进昆德拉所说的“生命不能承受之轻”。你是一个什么呢？生命算是个什么玩意儿呢？轻得称不出一点重量你可就要消失。我向L讨回那件东西，归途中的惶茫因年幼而无以名状，如今想来，分明就是为了一个“轻”字：珍宝转眼被处理成垃圾，一段生命轻得飘散了，没有了，以为是什么原来什么也不是，轻易、简单、灰飞烟灭。一段生命之轻，威胁了生命全面之重，惶茫往灵魂里渗透：是不是生命的所有段落都会落此下场呵？人的根本恐惧就在这个“轻”字上，比如歧视和漠视，比如嘲笑，比如穷人手里作废的股票，比如失恋和死亡。轻，最是可怕。 要求意义就是要求生命的重量。各种重量。各种重量在撞墙之时被真正测量。但很多生命的重量在死神的秤盘上还是轻，秤砣平衡在荒诞的准星上。因而得有一种重量，你愿意为之生也愿意为之死，愿意为之累，愿意在它的引力下耗尽性命。不是强言不悔，是清醒地从命。神圣是上帝对心魂的测量，是心魂被确认的重量。死亡降临时有一个仪式，灰和土都好，看往日轻轻地蒸发，但能听见，有什么东西沉沉地还在。不期还在现实中，只望还在美丽的位置上。我与L的情谊，可否还在美丽的位置上沉沉地有着重量？ 不要熄灭破墙而出的欲望，否则鼾声又起。 但要接受墙。 为了逃开墙，我曾走到一面墙下。我家附近有一座荒废的古园，围墙残败但仍坚固，失魂落魄的那些岁月里我摇着轮椅走到它跟前。四处无人，寂静悠久，寂静的我和寂静的墙之间，膨胀和盛开着冤屈。我用拳头打墙，用石头砍它，对着它落泪、喃喃咒骂，但是它轻轻掉落一点儿灰尘再无所动。天不变道亦不变。老柏树千年一日伸展着枝叶，云在天上走，鸟在云里飞，风踏草丛，野草一代一代落子生根。我转而祈求墙，双手合十，创造一种祷词或谶语，出声地诵念，求它给我死，要么还给我能走路的腿……但睁开眼，伟大的墙还是伟大地矗立，墙下呆坐一个不被神明过问的人。空旷的夕阳走来园中，若是昏昏睡去，梦里常掉进一眼枯井，井壁又高又滑，喊声在井里嗡嗡碰撞而已，没人能听见，井口上的风中也仍是寂静的冤屈。喊醒了，看看还是活着，喊声并没惊动谁，并不能惊动什么，墙上有青润的和干枯的台藓，有蜘蛛细巧的网，死在半路的蜗牛的身后拖一行鳞片似的脚印，有无名少年在那儿一遍遍记下的3.1415926…… 再这墙下，某个冬夜，我见过一个老人。记忆和印象之间总要闹出一些麻烦：记忆对我说未必是在这墙下，但印象总是把记忆中的那个老人搬来这墙下，说就是在这儿。……雪后，月光朦胧，车轮吱吱唧唧轧着雪路，是园中唯一的声响。这么走着，听见一缕悠沉的箫声远远传来，在老柏树摇落的雪雾中似有似无，尚不能识别那曲调时已觉其悠沉之音恰好碰住我的心绪。侧耳屏息，听出是《苏武牧羊》。曲终，心里正有些凄怆，忽觉墙影里一动，才发现一个老人盘腿端坐于墙下的石凳，黑衣白发，有些玄虚。雪地和月光，安静得也似非凡。竹箫又响，还是那首流放绝地、哀而不死的咏颂。原来箫声并不传自远处，就在那老人唇边。也许是力气不济，也许是这古曲一路至今光阴坎坷，箫声若断若续并不高亢，老人颤颤地吐纳之声亦可悉闻。一曲又尽，老人把箫管轻横腿上，双手摊放膝头，看不见他是否闭目。我惊诧而至感激，一遍遍听那箫声断处的空寂，以为是天谕或神来引领。 那夜的箫声和老人，多年在我心上，但猜不透其引领指向何处。仅仅让我活下去似不必这样神秘。直到有一天我又跟那墙说话，才听出那夜箫声是唱着“接受”，接受限制。接受残缺。接受苦难。接受墙的存在。哭和喊都是要逃离它，怒和骂都是要逃离它，恭维和跪拜还是想逃离它。失魂落魄的年月里我常去跟那墙谈话，是，说出声，以为这样才更虔诚或者郑重，出声地请求，也出声地责问，害怕惹怒它就又出声地道歉以及悔罪，所谓软硬兼施。但毫无作用，谈判必至破裂，我的一切条件它都不答应。墙，要你接受它，就这么一个意思反复申明，不卑不亢，直到你听。直到你不是更多地问它，而是它更多地问你，那谈话才称得上谈话。 我一直在写作，但一直觉得并不能写成什么，不管是作品还是作家还是主义。用笔和用电脑，都是对墙的谈话，是如吃喝拉撒睡一样必做的事。搬家搬得终于离那座古园远了，不能随便就去，此前就料到会怎样想念它，不想最为思恋的竟是那四面矗立的围墙；年久无人过问，记得那墙头的残瓦间长大过几棵小树。但不管何时何地，一闭眼，即刻就到那墙下。寂静的墙和寂静的我之间，野花膨胀着花蕾，不尽的路途在不尽的墙间延展，有很多事要慢慢对它谈，随手记下谓之写作。 1994年10月 史铁生]]></content>
      <categories>
        <category>散文分享</category>
      </categories>
      <tags>
        <tag>史铁生</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Connecting Targets to Tweets: Semantic Attention-Based Model for Target-Specific Stance Detection 笔记]]></title>
    <url>%2F2019%2F03%2F04%2FSemantic%20Attention-Based%20Model%20for%20Target-Specific%20Stance%20Detection%2F</url>
    <content type="text"><![CDATA[摘要研究组最近关注于针对特定目标的立场检测。针对隐式提及的目标，或根本没有提及的目标构建文本的向量表示依然是个很有挑战的工作。因此本文在语义层面上采用了最新的注意力机制，提出了一个基于双向 GRU-CNN 的模型。在 SemEval-2016 Task 6.A 上进行实验，此模型超过了 SOTA 词级别注意力机制的 GRU 与 SVM 方法。 论文原文Connecting Targets to Tweets: Semantic Attention-Based Model for Target-Specific Stance Detection WISE 2017, Yiwei Zhou, University of Warwick, Coventry, UK 1 简介针对具体目标的立场检测问题可以被定义为：给定一段文本 X 和一个目标 Y ，为每一个 X 针对给定的 Y ，从 {支持，反对，其他} 这三个类别中确定一个立场。目标可以是一个人，一个租住，一项政策，一场运动，一个产品等等。针对具体目标的力倡建祠与 aspect-level 的情感分析两个不同点：1. 可以从积极、消极、中性的发言中表达同一个立场。2. 立场检测中的目标并不一定出现在了文本里，可能是隐式提及的，可能根本没有提及。由于 Twitter 的文本通常很短，还有很多噪声，这个任务的主要挑战是针对具体目标进行立场检测的同时，使用很少一部分的上下文信息或额外信息。深度学习近期在机器翻译，问答系统，情感分析等领域取得了重大进展。但在 target-secific 的立场检测中并没有成功的应用深度学习，现有的结果甚至比传统的算法（手工特征工程 + SVM）更低。 不是文本中每个词对立场检测提供的信息量都一样，所以作者针对给定目标，使用了注意力机制。通过在语义层面引入目标信息与注意力机制，作者提出的模型超出了 SemEval-2016 中效果最好的模型。并且，不依赖额外收集的语料库来预训练 embedding，也不需要手工的特征工程。 2 神经网络模型在针对具体目标的立场检测中的应用本节，首先介绍两个基本模型，1.双向 GRU的模型，2. 在 biGRU 基础上叠加 CNN 的模型。在这基础上，通过在 token-level 与 semantic-level 上引入注意力机制，分别得到 AT-biGRU 模型 和 AS-biGRU-CNN 模型。最后介绍计算目标 embedding 的方式，以及模型训练的细节。 2.1 biGRU 模型通过引入门单元，GRU 可以解决梯度消失或梯度爆炸的问题。通过引入额外的记忆单元，GRU 可以捕捉到序列中的依赖关系。对于一个长度为 $N$ 的序列，[$x_1,x_2,…,x_N$]，通过以下公式，GRU 将其映射为一组隐层状态 [$h_1,h_2,…,h_N$] : $$ r_n = \sigma (W_r x_n + U_r h_{n-1} + b_r) $$ $$ z_n = \sigma (W_z x_n + U_z h_{n-1} + b_z) $$ $$ \tilde{h_n} = tanh(W_h x_n + U_h (r_n \odot h_{n-1}) + b_b) $$ $$ h_n = (1 - z_n) \odot h_{n-1} + z_n \odot \tilde{h_n} $$ 其中 $n \in {1,…,N}$；$r_n$ 是复位门，$z_n$ 是更新门；$\tilde h_n \in \mathbb{R}^{d_1}$ 为记忆单元，$h_n \in \mathbb{R}^{d_1}$ 为 GRU 的隐层状态；$x_n \in \mathbb{R}^{d_0}$ 为每个词的 embedding 向量；$W_r, W_z, W_h \in \mathbb{R}^{d_1 \times d_0}$ 和 $U_r, U_z, U_h \in \mathbb{R}^{d_1 \times d_1}$ 表示可训练的权重矩阵，$b_r, b_z, b_h \in \mathbb{R}^{d_1}$ 表示可训练的偏置项；$\sigma(·)$ 表示 sigmoid 函数；$\odot$ 表示 element-wise multiplication 。 通过使用将正向 GRU 与反向 GRU 得到的隐层状态值拼接，得到 [$\overrightarrow{h} | \overleftarrow{h}$]，捕捉双向信息。 在 biGRU 模型中，输入文本的向量表示为 s： $$ s = \overrightarrow{h_N} | \overleftarrow{h_1} $$ 2.2 biGRU-CNN 模型biGRU 将所有信息压缩在两个固定长度的隐层状态向量中，在存在长距离的依赖关系时可能会限制模型性能。根据所有 RNN 的隐层状态，输入到 CNN 网络中，得到一个动态的向量表示。具体的说，对拼接了 $k$ 次的隐层状态 $h_{h:i+k-1} \in \mathbb{R}^{2kd_1}$ 添加一个过滤器权重 $w_f \in \mathbb{R}^{2kd_1}$，计算得到 $c_i$： $$ c_i = f(w^T_f h_{i:i+k-1} + b_f) $$ 其中 $f$ 是 ReLu 函数，$b_f \in \mathbb{R}$ 是偏置项。对于所有的特征集合 $\textbf{c}$ = $(c_1,c_2,…,c_{N-k+1})$。通过 max 函数获取最重要的特征 $\hat{c}$： $$ \hat{c} =max{\textbf{c}}$$ 2.3 AT-biGRU 模型机器翻译中首先提出的 token-level 的注意力机制，允许神经网络自动的在源句子中寻找与预测对应词相关的 token，并且遮蔽不相关的 token。本文中，为了使 biGRU 针对跟定目标可以自动寻找对应的部分来觉得这段文本的立场，作者提出了融合注意力机制的模型，如图 Fig.1： 在 AT-biGRU 模型中，文本的向量表示 $s$ 是隐层状态的加权和： $$ s = \sum^n_{n=1} \alpha_n h_n $$ 在上面的等式中，权重 $\alpha_n$ 的计算是： $$ \alpha_n = \frac{exp(e_n)}{\sum^n_{n=1}exp(e_n)}$$ 其中 $e_n \in \mathbb{R}$ 是用 $h_n$ 和目标 embedding $q$ 作为输入，通过多层感知机进行计算： $$ e_n = att(h_n, q) = w^T_m(tanh(W_{ah}h_n + W_{aq} q + b_a)) + b_m$$ 其中 $W_{ah} \in \mathbb{R}^{2d_1 \times 2d_1}; W_{aq} \in \mathbb{R}^{2d_1 \times 2d_2};b_a,w_m \in \mathbb{R}^{2d_1};b_m \in \mathbb{R}$ 计算 token-level 的注意力层优化参数。在 2.5 节，会介绍各种根据目标的词向量，生成目标 embedding $q \in \mathbb{R}^{b_2}$ 的方式。 2.4 AS-biGRU-CNN 模型通过将 biGRUN-CNN 中的 $h_n$ 用融合了目标注意力的 ${h’}_n$，得到 AS-biGRU-CNN 模型。其中 ${h’}_n$ 的计算为：$$ {h’}_n = a_n \odot h_n $$其中 $a_n$ 的计算方式与 2.3 相同。 2.5 目标 Embedding作者使用 biGRU 对训练目标词向量，得到 $q$。同时与常见的用目标词向量的平均值作为目标 embedding 进行了对比。 2.6 模型训练对文本的向量表示 $s$ 添加 softmax 层，得到对应每个立场的概率。对于文本 $X$ ，给定特定目标 $Y$ 计算所属立场类别的概率 $z$，用 $P(z|X,Y)$。表示。模型目标为 $z’$ ： $$ z' = argmax_{z \in \textbf{z}}P(z|X,Y) $$ 所有模型都是连续可导的，采用端到端的训练方法，用标准的反向传播训练。使用交叉熵作为损失函数。 3 实验结果参数设置： 使用 Glove 100 的 word embedding on Wikipedia。 biGRU 纬度为 128 对embedding 层的 dropout 为 0.2，对 GRU 的 dropout 为 0.3 ，对CNN 的 droput 为 0.5。 CNN 的 $k \in {3,4,5}$ 使用 Adam，momentum 参数为 0.9 和 0.999。 mini-batch size 为 16 模型代码见：https://github.com/zhouyiwei/tsdSemEval 数据有 5 个主题，针对每个主题分别训练模型，得到结果如下： 5 总结针对 target-specific 的目标检测提出了 AS-biGRU-CNN 模型。主要贡献是首次引入 target 信息，并融合注意力机制。模型没有使用太多的额外信息，扩展性强。 Notes实验中作者发现共享 target 和 tweet 的 embedding 效果比二者分别使用各自的 embedding 效果好。 实验发现用直接对 word embedding 平均替代 biGRU 训练 target 的 word embedding 后，AT-biGRU 从 67.97 提升到了 68.30，而 AS-biGRU-CNN 从 69.42 降低到了 68.35。作者解释如下（我没读懂，难受） One possible explanation could be that a simple averaging approach is insufficient to capture the semantic meanings of the targets, thus for the biGRU-CNN model, which has stronger expressive power than the biGRU model in target-specific Stance Detection, it is helpful to use more flexible target embeddings to perform complex inference. However, for the AT-biGRU model, the target embeddings generated by biGRU surpass its capability to learn and generalise. This is also the reason why stacking the CNN structure on top ofthe AT-biGRU model cannot help to improve the performance, as it does in the AS-biGRU-CNN model. 由于数据集有 5 个主题，每个主题下的数据量不同，而且样本中各个立场的比例也不均匀。作者假设在各个立场上的上下文可以 「jointly modelling the interaction between stances and contexts of all the available targets」。所以作者尝试了用一个模型在这 5 个立场上训练。结果发现数据量少的话题上 F1 有提升，数据量多的话题上 F1 有下降。对于样本不足，jointly 训练有帮助，对于样本充足，额外信息反而会让模型效果下降。 所有基于网络的模型都比 SVM 效果好，也许是因为网络模型使用的是连续向量表示文本，SVM 使用的是根据特征工程得到的离散的特征。连续的词向量表示可以时词之间共容易发生信息传递。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>立场检测</tag>
        <tag>神经网络</tag>
        <tag>注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stance Classification with Target-Specific Neural Attention Networks 笔记]]></title>
    <url>%2F2019%2F03%2F03%2FStance%20Classification%20with%20Target-Specific%20Neural%20Attention%20Networks%2F</url>
    <content type="text"><![CDATA[摘要立场检测任务，是给定一段文本与一个特定目标，得到文本编写者对于这个目标所持的立场。这个任务与 aspect-level 的情感分类最主要的区别是，给定的目标可能没有在文本中显式的出现。针对这个问题，作者 Du 设计了一种基于神经网络的模型，将目标信息与注意力机制结合，定位出文本中对立场检测重要的部分。Du 的模型分别在中文与英文数据集上进行了测试，得到了 SOTA 的效果。 论文原文Stance Classification with Target-Specific Neural Attention Networks IJCAI 2017, Jiachen Du et al, Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen, China 1 简介随着网络的发展，越来越多的人在线上媒体表达自己的观点。然而，比起判断一段文本的极性，很多的应用场景里更关注与判断文本作者对某个目标的立场。比如美国大选。对于立场分析的早期研究关注于对论坛和新闻的立场检测。随着社交媒体的发展，更多的人关注于在微博平台上的应用，这有很多应用价值。立场检测被定义为一个分类问题，为给定的一段文本与一个特定立场，指派 {支持，反对，其他} 其中的一个标签。在已有的研究中，有很多使用特征工程手工从中抽取特征的，也有使用 CNN/RNN 等经典网络结构的。但这些方法都忽略了目标信息。因此，作者 Du，提出了他妈的模型 Target-specific Attentional Network (TAN) 将目标信息应用在立场检测中。他们的模型利用了最新的针对目标信息的注意力特征抽取器，首先，将目标与文本的词向量连接起来，然后用一个全连接网络学习注意力信号量，让分类器关关注于重要的部分。实验结果显示，在中文与英文数据集上，TAN 模型取得了 SOTA 的结果。Du 工作的主要贡献有： 提出了针对目标的注意力抽取计算方式。使得他们的模型可以根据目标信息从文本中抽取出最重要的部分。 提出了一个有监督模型，TAN。这个模型结合了注意力与 LSTM。 在 Semeval-2016 (英文数据集) 和 NLPCC-2016 (中文数据集) 上进行了实验，TAN 模型超过了最好水平。并且通过对注意力层的可视化，说明了为何这么模型有效果。 2 相关工作Augenstein et al. EMNLP 2016 对文本和目标分别使用一个 RNN 模型来做立场检测。同时还使用了大量的无标记 Twitter 预料来预测与任务相关的 hashtag，对 word embedding 进行初始化。 在更通用的情感分析领域，Tai et al, ACL 2015 使用了树形结构的 RNN。 Yang et al., ACL 2016 在 seq2seq 的文档层面的分类任务中使用了注意力机制模型。到目前为止，还没有为立场分类提出过相应的注意力层计算方式。 3 模型如前面所提到的，同时考虑文本信息与目标信息，有可能会提高对立场检测的效果。在这个动机下，作者提出了结合 RNN 与 注意力机制的神经网络模型。称作 Target-specific Attention Neural Network (TAN)。模型的结构如下： 模型主要有两部分，一个使用 RNN 的特征抽取器，以及一个全连接层计算分配注意力后的特征。 3.1 RNN 部分在他们的模型中，使用 LSTM 中每个 step 的隐层状态，对应文本中的词。模型中使用了双向 LSTM 。连接正向与反向 LSTM 的隐层状态值来表示文本中的每个词。 3.2 结合目标信息的词向量一个长度为 $N$ 的目标序列，用 $[z_1, z_2, …, Z_N]$ 表示，其中 $\mathbb{R}^{d’}$ 是 $d’$ 维的向量。由于 word embedding 可以展示的是词的线性结构，因此在词级别上进行向量相加依然保有一定的语义，作者 Du 用 $\tilde{z}$ 表示更压缩的目标信息： $$ \bar{z} = \frac{1}{N} \sum^{N}_{n=1} z_n $$ 因此，结合目标 $Z$ 的词向量为： $$ e^{z}_{t} = x_t \oplus \bar{z} $$ 其中 $\oplus$ 表示向量的连接操作。因此，如果文本词向量的维度是 $d$，目标信息向量的维度是 $d’$ ，结合目标信息的词向量 $e^z_t$ 的维度是 $(d + d’)$ 。 3.3 目标信息的注意力权重计算作者用全连接层训练注意力权重： $$ {a’}_t = W_a e_t^z + b_a $$ 其中 $W_a$ 和 $b_a$ 是注意力抽取中的权重与偏置项。 为了获得更稳定的注意力值，作者将注意力向量 $[{a’}_1, {a’}_2,…, {a’}_T]$ 输入到 softmax 层来得到最终的注意力值： $$ a_t = softmax(a_t) = \frac{e^{{a'}_t}}{\sum^T_{i=1}e^{{a'}_i}} $$ 3.4 立场分类用注意力值 $a_t$ 和对应的文本 RNN 隐层状态 $h_t$ 的乘积来表示第 $t$ 个字。用所有字的平均值来表示整个序列： $$ s = \frac{1}{T} \sum^{T-1}_{t=1} a_t h_t $$ 其中 $s \in \mathbb{R}^d$。用这个向量作为序列分类的特征。 $$ p = softamx(W_{clf}s + b_{clf}) $$ 其中 $p \in \mathbb{R}^C$，是这个向量对应立场的预测概率。这里 $C$ 是立场类别的数量，$W_{clf}$ 和 $b_{clf}$ 是分类层的训练参数。 3.5 模型训练作者使用交叉熵损失损失来端到端的训练模型。对于一组训练数据 ${x^i, z^i, y^i}$，其中 $x^i$ 是第 $i$ 个需要预测的文本，$z^i$ 是给定的目标，$y_i$ 是立场类别的 one-hot 表示。模型可以表示为一个黑箱函数 $f(x,z)$，输出是一个向量，对应各个立场的概率。训练目标是最小化损失函数： $$\mathcal{L} = - \sum_i \sum_j y_j^i log f_j(X_i, z_i) + \lambda | \theta |^2 $$ 其中 $\lambda | \theta |^2$ 是 L2 正则项。模型是一组标准的 LSTM 与 softmax 分类器，只多出一组 ${W_a, b_a}$ 的注意力层。 4 模型评估本节先介绍实验设置，然后与其他 baseline 进行比较，最后可视化注意力层，表明注意力抽取层的有效性。 4.1 实验设置数据集 为了检验模型效果，作者在分别在中文与英文的立场检测任务上进行了实验。 英文数据集 Semeval-2016 Task 6 。这个数据集中有大约 4,000 条推特，每条推特有属于一个话题，共有 5 个话题 「Atheism, Climate Change is a Real Concern, Feminist Movement, Hillary Clinton, Legalization of Abortion」。Table 1 是这个数据集的统计信息。 中文数据集 NLPCC-2016 中文立场检测任务。这个数据集与 Semeval-2016 相似。共有 3,000 跳中文微博，5 个目标，3 个立场标签。对于每个目标，有 600 个训练数据和 200 个测试数据。Table 2 展示了这个数据集的统计信息。 评价指标 使用在 Favor（支持）和 Against（反对）类别上的 F1-score 作为评价指标： $$ F_{Favor} = \frac{2P_{Favor}R_{Favor}}{P_{Favor}{R_{Favor}}} $$ $$ F_{Against} = \frac{2P_{Against}R_{Against}}{P_{Against}{R_{Against}}} $$ 其中 $P$ 和 $R$ 是 precision（准确率） 和 recall（召回率）。然后对 $F_{Favor}$ 和 $F_{Against}$ 进行平均，作为最终的评价指标： $$ F_{average} =\frac{F_{Favor} + F_{Against}}{2} $$ 注意，最终的评价指标忽略了 {None} 这个类别。我们只关心在 Favor 和 Against 类别上的效果。 Baselines 作者对比的 baseline 方法有： Neural Bag-Of-Words (NBOW): 对文本的词向量家和，用 softmax 进行分类。 LSTM : 没有目标信息与注意力机制的网络 LSTM : 结合了目标信息的 embedding TOP: 各任务上效果最好的模型。 Semeval-2016: MITER。模型有两个 RNN，第一个 RNN 在无标签的推特数据训练 task 相关的 hashtag 的 embedding，用来初始化第二个 RNN。 用对应数据集训练第二个 RNN 进行分类。Augenstein et al., 2016) NLPCC-2016： RUC_MMC。这个方法中分别对 5 个目标训练了对应的 5 个模型。每个模型中有 5 中手工抽取的特征，然后用 SVM 和 随机森林进行分类 Xu et al., 2016 训练设置 对每个目标分别训练一个模型。将所有模型的预测结果拼起来，作为模型的最终结果。所有模型使用相同的超参数设置。在 5-fold 交叉验证上选择最优参数。模型 Embedding 使用 Mikolov et al., 2013 的方法进行初始化。从 Twitter 和新浪微博上抓取无标签数据进行训练。使用 U (-0.01, 0.01) 的均匀分布进行初始化。词向量维度为 300， LSTM 电源的纬度为 100.使用 Adam 作为优化器，learning rate 为 $5e-4$，$\beta_1$ 为 0.9，$\beta_2$ 为 0.999，$\epsilon$ 为 $1e-8$ 。所有模型使用大小为 50 的 mini-batch 进行训练。 4.2 结果 中文比英文高，可能的原因是中文数据集更均匀。NBOW 和 LSTM 未引入目标信息，效果不佳。 引入目标信息的 $LSTM_E$ 分数有 3.03% 左右的提升，说明目标信息有效果。$TAN$ 增加注意力后，比 $LSTM_E$ 又高出 3.54% 左右，说明注意力机制有效。TAN 模型在中文与英文上都超过了最佳模型，说明 TAN 是与语言类型无关的模型。 4.3 定性分析：学习曲线 中文与英文数据集上 TAN 均在 3 个迭代后收敛，而标准 LSTM 需要 5 次迭代。并且 TAN 的 loss 更低。说明 TAN 模型在准确度与时间复杂度上都更优。 注意力可视化 在注意力的上方写出了对应的目标，情感，立场，右侧为 TAN 和标准 LSTM 给出的预测结果。可视化表明注意力机制可以给与目标信息更相关的词赋予更高的权重。例子中的情感与立场信息并不一致，说明立场分类与情感分类有本质上的区别。TAN 的预测结果是正确的， LSTM 的预测结果是错误的。 4.4 错误分析 上图分别给出了一个英文与中文的错误分类例子。英文例子中，是一句从圣经中引用的话。因此正确的预测立场需要一些额外的背景知识。中文例子中，生二胎会引起对看病上学的担忧，所以文本作者持反对立场。模型正确的分配的注意力，但给出了错误的结果。 5 总结本篇论文提出了一个基于神经网络的立场检测模型。本文的主要贡献是结合立场信息与注意力机制。实验证明所提出的方法超过了 baseline。注意力可视化说明所提出的模型有能力抽取出对立场检测重要的文本。进一步的工作，将关注于在其他的 SOTA 模型中引入注意力机制。以及用更灵活的方法引入额外的信息来提升立场检测的效果。 Notes文章结构清晰，模型思路直接。进一步方向是引入额外信息，以及使用更好的特征抽取方式。模型实现简单，推荐作为 baseline 使用。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>立场检测</tag>
        <tag>神经网络</tag>
        <tag>注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stance Detection with Hierarchical Attention Network 笔记]]></title>
    <url>%2F2019%2F02%2F28%2FStance%20Detection%20with%20Hierarchical%20Attention%20Network%2F</url>
    <content type="text"><![CDATA[论文原文Stance Detection with Hierarchical Attention Network COLING 2018, Qingying Sun et al, Soochow University, Suzhou, China 任务对文本针对特定话题或实体的支持/反对态度进行分类。 模型句子中除了文字所包含的信息，其语言学信息同样对立场识别有帮助。为将这两部分特征结合，作者提出了多层注意力网络模型 (Hierarchical Attention Network)。语言学注意力层 (linguistic attention) 调节文字信息与各个语言学特征的关系。超注意力层 (hyper attention) 调节各个特征的权重。 结构 各个特征的计算文本特征$$ X = (w_1, w_2, …, w_n) $$设句子为 $X$，由词 $w_1,…w_n$ 组成，其中 $n$ 为词的个数。用 skip-gram 算法 预训练的 word embedding，将句中的词转为对应的词向量。使用 LSTM 训练词向量，其中对于第 $t$ 个词，其在LSTM中的隐层状态值为 $h_(t)$，$h_(t) = LSTM (w_t, h_(t-1))$。用最后一个的隐层状态代表文本特征 $H = h_n$，LSTM 的可训练参数都是随机初始化的。 语言学特征情感特征 例句1： 如果我们真诚的希望国家保持进步，希拉里是我们最好的选择。（实体：希拉里·克林顿） 例句2： 当砍完了最后一棵树，吃掉了最后一只鱼，污染了最后一条河流，你终将意识到，钱不能用来吃。 （话题：增加对气候变化的关注度） 情感与立场紧密相关，例句1是积极的，支持实体希拉里；例句2消极的，但同样表达了对话题的支持。由于句子的极性（积极/消极）与其立场没有直接关系，作者选择用神经网络来学习句子的情感特征。通过查找 MPQA subjective lexicon，抽取出带有极性的词语，组成情感特征序列。$$ X^{(sent)} = { x_1^s, x_2^s, …, x_m^s} $$ MPQA 数据格式 type=strongsubj len=1 word1=abuse pos1=verb stemmed1=y priorpolarity=negative 使用 LSTM 训练情感特征序列，用最后一个隐层状态作为情感特征 $H^{(sent)} = LSTM(x^s_m, h_{m-1})$。 依存特征依存特征可以表示词与词之间的关系。 句中「murder-never-necessity」就是与立场相关的一个依存关系。通过使用 Stanford Parser，作者提取出所有与 noun, verb, adjective, adverb and negation 有关联的关系，如「nsubj」，「acl」，「dobj」等。用提取出的关系对组成依存序列，$$ X^{(dep)} = {x^d_1, X^d_2, …, X^d_m} = {x_1 \oplus x_3, …, x_i \oplus x_j}$$其中 $x_i^d = x_j \oplus x_k$ 表示一个依存关系对。使用 LSTM 训练依存特征序列，用最后一个隐层状态作为依存特征 $H^{(dep)} = LSTM(x_m^d, h_{m-1})$。 信念特征对于特定话题/实体的立场，其背后是一个人的信念。如果可以从句子中得到与信念有关的部分，这会有助于判断句子的立场。 reason数据样例：B14.data.rsn Most issues like this, such as sex between minors and alcohol, come down to one thing: it’s your choice. If you want to ruin your life, be my guest. It isn’t the goverment’s job to control that. Label##p-right -&gt; Prohibition violates human rights Line##Most issues like this, such as sex between minors and alcohol, come down to one thing: it’s your choice. 作者将 Hasan and Ng(2014) 的 reason 数据简化为二分类数据，使用 libSVM 训练得到一个可以区分句子是否与信念相关的 SVM。通过这个 SVM，得到文本内的信念特征序列$$ X^{(argument)} = {x^a_1, x^a_2, …, X^a_m} $$使用 LSTM 训练信念特征序列，用最后一个隐层状态作为信念特征 $H^{(argument)} = LSTM(x^a_m, h_{m-1})$。 语言学特征的注意力层对于文本中的每个词，通过注意力机制，对语言学特征计算一个对应的权重 $\alpha$ 。$$ \alpha_{ij} = \frac{exp(w_{ij})}{\sum_{j=1}^{n}exp(w_{ij})} $$ $$ w_{ij} = tanh(W^T[h_j:l_i] + b) $$ 其中 $l_i \in {H^{(sent)}, H^{(dep)}, H^{(argument)}}$, 为相应的语言学特征。$h_j \in H$，第 $j$ 个词的文本特征。例如，当计算情感特征 $l_1 = H^{(sent)}$，对于文本特征 $h_1$ 的对应权重 $\alpha_{11}$。文本中所有词的文本特征 $H$ 与对应语言学特征的加权和，作为该语言学特征的值 $q_i$ $$ q_i = \sum_{j=1}^{n} \alpha_{ij} h_j $$ 超注意力层不同的文本，在三个语言学特征上的值应当不同。根据不同的文档，对各个特征赋予不同的权重。$$ w_j = tanh(W^t V_j + b) $$$$ \beta_j = \frac{exp(w_j)}{\sum_{j=1}^4 exp(w_j)} $$$$ q = \sum_{j=1}^{4} \beta_j k_j, \quad k_j \in {H, q_1, q_2, q_3}$$ $V$ 为不同文档的权重矩阵。 训练使用交叉熵作为损失函数，对训练的参数加 L2 约束。 $$ L(\Theta) = - \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^m y_{ij} log p_{ij} + \frac{\lambda}{2} ||{\Theta}||^2 $$ 神经网络的 droprate 设置为 0.25，embedding 的维度为 300，attention 的维度为 300，优化器使用 Adam，使用标准的反向传播。从训练集中抽取 10%的数据作为开发数据，学习率和其他超参数在开发数据上调整。 评测指标使用 F1-score 作为评测指标， $$ F_{favor} = \frac{2 \times P_{favor} \times R_{favor}} {P_{favor} + R_{favor}} $$$$ F_{against} = \frac{2 \times P_{against} \times R_{against}} {P_{against} + R_{against}} $$ 其中 P 和 R 分别是准确率和召回率。然后计算 $F_{favor}$ 和 $F_{against}$ 的平均作为最后的指标： $$ F_{avg} = \frac{F_{favor} + F_{against}} {2}$$ 在话题上的平均记为 F score($MacF_{avg}$)$，在各个立场上的平均记为 F score ($MicF_{avg}$)。结果对比如下： 语言学特征对模型效果的影响 由上表可知所有的语言学信息结合注意力层后，对立场检测都有帮助。 网络结构对模型效果的影响 -Hyper 表示去除 Hyper Attention 层，-Hyper,-Ling 表示同时去除 Hyper Attention 层和 linguistic attention 层，直接将语言学特征与文本特征拼接。与 LSTM 对比表明，语言学特征对与立场分析的确有提升。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>立场检测</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
</search>
